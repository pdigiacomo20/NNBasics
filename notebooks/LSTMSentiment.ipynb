{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "from trax import shapes\n",
    "from trax.fastmath import numpy as jnp  # For use in defining new layer types.\n",
    "from trax.shapes import ShapeDtype\n",
    "from trax.shapes import signature\n",
    "from trax import supervised as ts\n",
    "from trax.supervised import training\n",
    "import trax\n",
    "from trax.fastmath import jax\n",
    "from trax.fastmath import jit\n",
    "import random as rnd\n",
    "import trax.data as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_imdb = td.TFDS('imdb_reviews', keys=('text', 'label'), train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pd/NNBasics/venv_NNBasics/bin/python\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = td.Serial(\n",
    "  td.TFDS('imdb_reviews', keys=('text', 'label'), train=True),\n",
    "  td.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n",
    "  td.Shuffle(),\n",
    "  td.FilterByLength(max_length=2048, length_keys=[0]),\n",
    "  td.BucketByLength(boundaries=[  32, 128, 512, 2048],\n",
    "                      batch_sizes=[128,  32,   8,    2, 1],\n",
    "                      length_keys=[0]),\n",
    "  td.AddLossWeights()\n",
    ")\n",
    "\n",
    "# inputs_test = td.Serial(\n",
    "#   td.TFDS('imdb_reviews', keys=('text', 'label'), train=False),\n",
    "#   td.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n",
    "#   td.Shuffle(),\n",
    "#   td.FilterByLength(max_length=2048, length_keys=[0]),\n",
    "#   td.BucketByLength(boundaries=[  32, 128, 512, 2048],\n",
    "#                       batch_sizes=[128,  32,   8,    2, 1],\n",
    "#                       length_keys=[0]),\n",
    "#   td.AddLossWeights()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not write evaluation metrics, because output_dir is None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 11:44:57.750446: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-12-15 11:44:57.770026: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "def get_tasks(gen):\n",
    "\n",
    "    tt = training.TrainTask(\n",
    "        labeled_data=inputs(),\n",
    "        loss_layer = tl.WeightedCategoryCrossEntropy(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        n_steps_per_checkpoint=5,\n",
    "    )\n",
    "\n",
    "    et = training.EvalTask(\n",
    "        labeled_data=inputs(),        \n",
    "        metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],\n",
    "    )\n",
    "\n",
    "    return tt,et\n",
    "\n",
    "DenseSentiment = tl.Serial(\n",
    "    tl.Embedding(vocab_size=8000,d_feature=256),\n",
    "    tl.Mean(axis=1),\n",
    "    tl.Dense(n_units = 2),\n",
    "    tl.LogSoftmax()\n",
    ")\n",
    "\n",
    "tt,et = get_tasks(inputs())\n",
    "\n",
    "training_loop = training.Loop( \n",
    "                            DenseSentiment, # The learning model\n",
    "                            tt, # The training task\n",
    "                            eval_tasks= et, # The evaluation task\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step     30: Ran 2 train steps in 0.07 secs\n",
      "Step     30: train WeightedCategoryCrossEntropy |  0.68922001\n",
      "Step     30: eval  WeightedCategoryCrossEntropy |  0.71968335\n",
      "Step     30: eval      WeightedCategoryAccuracy |  0.37500000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.375),\n",
       " (5, 0.5),\n",
       " (10, 0.5),\n",
       " (15, 1.0),\n",
       " (20, 0.5),\n",
       " (25, 0.625),\n",
       " (30, 0.375)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop.run(n_steps = 2)\n",
    "#training_loop.history.get('train','metrics/WeightedCategoryCrossEntropy')\n",
    "training_loop.history.get('eval','metrics/WeightedCategoryAccuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class History in module trax.supervised.history:\n",
      "\n",
      "class History(builtins.object)\n",
      " |  History of metrics.\n",
      " |  \n",
      " |  History contains the metrics recorded during training and evaluation.\n",
      " |  Save data with history.append and get a sequence of data by calling\n",
      " |  history.get.\n",
      " |  \n",
      " |  For example:\n",
      " |  history.append('train', 'metrics/accuracy', 1, 0.04)\n",
      " |  history.append('train', 'metrics/accuracy', 1000, 0.31)\n",
      " |  history.get('train', 'metrics/accuracy')\n",
      " |  # returns [(1, 0.04), (1000, 0.31)]\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  append(self, mode, metric, step, value)\n",
      " |      Append (step, value) pair to history for the given mode and metric.\n",
      " |  \n",
      " |  get(self, mode, metric)\n",
      " |      Get the history for the given metric and mode.\n",
      " |  \n",
      " |  metrics_for_mode(self, mode)\n",
      " |      Metrics available for a given mode.\n",
      " |  \n",
      " |  to_dict(self)\n",
      " |      Serializes this instance to a Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_dict(json_object) from builtins.type\n",
      " |      Constructs a `History` from a Python dictionary of parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  modes\n",
      " |      Current tracked modes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trax.supervised.history.History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ==: 'DeviceArray' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_loop\u001b[39m.\u001b[39;49mload_checkpoint(directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mout1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/NNBasics/venv_NNBasics/lib/python3.10/site-packages/trax/supervised/training.py:944\u001b[0m, in \u001b[0;36mLoop.load_checkpoint\u001b[0;34m(self, directory, filename)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[39mfor\u001b[39;00m (trainer, slots) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_per_task, d[\u001b[39m'\u001b[39m\u001b[39mslots_per_task\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    941\u001b[0m   matched_flat_slots \u001b[39m=\u001b[39m _match_by_shape(\n\u001b[1;32m    942\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_bits(_flatten_and_remove_empty(trainer\u001b[39m.\u001b[39mslots)),\n\u001b[1;32m    943\u001b[0m       _flatten_and_remove_empty(slots))\n\u001b[0;32m--> 944\u001b[0m   matched_slots, _ \u001b[39m=\u001b[39m fastmath\u001b[39m.\u001b[39;49mtree_unflatten(\n\u001b[1;32m    945\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_from_bits(matched_flat_slots),\n\u001b[1;32m    946\u001b[0m       trainer\u001b[39m.\u001b[39;49mslots, copy_from_tree\u001b[39m=\u001b[39;49m[\u001b[39mNone\u001b[39;49;00m, ()])\n\u001b[1;32m    947\u001b[0m   trainer\u001b[39m.\u001b[39mslots \u001b[39m=\u001b[39m matched_slots\n\u001b[1;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step \u001b[39m=\u001b[39m d[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/NNBasics/venv_NNBasics/lib/python3.10/site-packages/trax/fastmath/numpy.py:244\u001b[0m, in \u001b[0;36mtree_unflatten\u001b[0;34m(flat, tree, copy_from_tree)\u001b[0m\n\u001b[1;32m    242\u001b[0m new_tree, rest \u001b[39m=\u001b[39m [], flat\n\u001b[1;32m    243\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tree:\n\u001b[0;32m--> 244\u001b[0m   new_t, rest \u001b[39m=\u001b[39m tree_unflatten(rest, t, copy_from_tree\u001b[39m=\u001b[39;49mcopy_from_tree)\n\u001b[1;32m    245\u001b[0m   new_tree\u001b[39m.\u001b[39mappend(new_t)\n\u001b[1;32m    246\u001b[0m new_tree \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(new_tree) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tree, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m new_tree\n",
      "File \u001b[0;32m~/NNBasics/venv_NNBasics/lib/python3.10/site-packages/trax/fastmath/numpy.py:244\u001b[0m, in \u001b[0;36mtree_unflatten\u001b[0;34m(flat, tree, copy_from_tree)\u001b[0m\n\u001b[1;32m    242\u001b[0m new_tree, rest \u001b[39m=\u001b[39m [], flat\n\u001b[1;32m    243\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tree:\n\u001b[0;32m--> 244\u001b[0m   new_t, rest \u001b[39m=\u001b[39m tree_unflatten(rest, t, copy_from_tree\u001b[39m=\u001b[39;49mcopy_from_tree)\n\u001b[1;32m    245\u001b[0m   new_tree\u001b[39m.\u001b[39mappend(new_t)\n\u001b[1;32m    246\u001b[0m new_tree \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(new_tree) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tree, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m new_tree\n",
      "File \u001b[0;32m~/NNBasics/venv_NNBasics/lib/python3.10/site-packages/trax/fastmath/numpy.py:239\u001b[0m, in \u001b[0;36mtree_unflatten\u001b[0;34m(flat, tree, copy_from_tree)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_unflatten\u001b[39m(flat, tree, copy_from_tree\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    217\u001b[0m   \u001b[39m\"\"\"Unflatten a list into a tree given the tree shape as second argument.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m    more were provided than the number of leaves of tree (useful for recursion).\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m   \u001b[39mif\u001b[39;00m copy_from_tree \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m tree \u001b[39min\u001b[39;49;00m copy_from_tree:\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m tree, flat\n\u001b[1;32m    241\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tree, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[0;32m~/NNBasics/venv_NNBasics/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:4938\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4936\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   4937\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[0;32m-> 4938\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsupported operand type(s) for \u001b[39m\u001b[39m{\u001b[39;00mopchar\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4939\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4940\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ==: 'DeviceArray' and 'tuple'"
     ]
    }
   ],
   "source": [
    "training_loop.load_checkpoint(directory='out1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip\n",
    "with gzip.open('out1/model_90.pkl.gz','rb') as f:\n",
    "    dat = pickle.load(f)\n",
    "    dir(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flat_weights</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flat_state</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flat_eval_state</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>history</td>\n",
       "      <td>{'_values': {'train': {'metrics/WeightedCatego...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>slots_per_task</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>input_signature</td>\n",
       "      <td>(ShapeDtype{shape:(2, 1024), dtype:int64},)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>version_timestamp</td>\n",
       "      <td>Mar-10-2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                                                  1\n",
       "0               step                                                 90\n",
       "1       flat_weights                                                  2\n",
       "2         flat_state                                                 []\n",
       "3    flat_eval_state                                                 []\n",
       "4            history  {'_values': {'train': {'metrics/WeightedCatego...\n",
       "5     slots_per_task                                                  2\n",
       "6    input_signature        (ShapeDtype{shape:(2, 1024), dtype:int64},)\n",
       "7  version_timestamp                                        Mar-10-2021"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dat.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method run in module trax.supervised.training:\n",
      "\n",
      "run(n_steps=1) method of trax.supervised.training.Loop instance\n",
      "    Runs this training loop for n steps.\n",
      "    \n",
      "    Optionally runs evals and saves checkpoints at specified points.\n",
      "    \n",
      "    Args:\n",
      "      n_steps: Stop training after completing n steps.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(training_loop.run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_NNBasics",
   "language": "python",
   "name": "venv_nnbasics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54933d5d0454cb54ea8cf4e7b3c099269c704adbee2a5b35f5a5ea4d0f5219ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
