{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Transfer Learning for Sentiment Analysis\n",
    "**Objective:** Provide a framework to perform transfer learning using the pre-trained distilBERT model, \n",
    "allowing options for fine-tuning the distilBERT model or simply use its outputs as features. In this example, \n",
    "we use a dataset of Yelp reviews and build a sentiment classifier to identify whether a \n",
    "review is 1 or 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 16:52:36.615128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-11 16:52:37.562516: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-11 16:52:39.212728: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-11 16:52:39.213027: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-11 16:52:39.213043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "**Summary:** parse Yelp reviews for the review text and number of stars associated with that review.\n",
    "Only parse reviews with low or high stars, and ensure that we have an equal number of low and high star reviews. Low star reviews have 1 or 2 stars, and high star reviews have 5 stars.<br /> \n",
    "- *lowstar_review_limit*: once we parse this number of low star reviews have been parsed. Break from processing. Typically there are more high star than low star reviews so the total number of reviews read in will be twice this number. <br /> \n",
    "- *review_limit*: once a total of this number of reviews have been parsed, stop reading in more.<br /> \n",
    "- *sample_per_cat*: sample this many low star and high star reviews respectively from what is parsed.<br /> \n",
    "- *max_num_words*: only parse reviews with number of words less than this length <br /> \n",
    "- Note that here we provide training results over a small dataset using the CPU so that quick testing and tuning can occur before scaling to larger compute resources on a GPU.\n",
    "[Download the data](https://www.yelp.com/dataset/documentation/main) <br /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of low star reviews:1024\n",
      "1151    Pretty awkward experience here. Sat down an ho...\n",
      "1771    In addition to my previous review, my husband ...\n",
      "1859    Went there for brunch today. Unfortunately, th...\n",
      "1414    Don't go!!!! Walk a couple more feet and go to...\n",
      "1764    Delivery issues caused our pizza to be over 30...\n",
      "Name: _text, dtype: object\n",
      "Number of 5 star reviews:1024\n",
      "1010    Go here for breakfast almost every weekend. Gr...\n",
      "439     I had the classic burger and fries. Overall pl...\n",
      "289     This place is great!  I've had the Pho a few t...\n",
      "327     Food and service was great. I had the pork cho...\n",
      "16      Came in on a Friday night and got immediate se...\n",
      "Name: _text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#read-in data\n",
    "lowstar_review_limit = 1024\n",
    "review_limit = np.inf\n",
    "sample_per_cat = 1024\n",
    "max_num_words = 50\n",
    "\n",
    "\n",
    "path = \"/home/pd/datasets/yelp_reviews/yelp_reviews.json\"\n",
    "review_fields_wanted = ['text','lowstar']\n",
    "rev = pd.DataFrame(columns=review_fields_wanted)\n",
    "with open(path,encoding='utf-8') as d:\n",
    "    counter = 0\n",
    "    lowstar_counter = 0\n",
    "    for line in d:\n",
    "        L = json.loads(line)\n",
    "        lowstar = L['stars'] == 1 or (L['stars'] == 2)\n",
    "        fivestar = L['stars'] == 5\n",
    "        not1or5 = not(lowstar or fivestar)\n",
    "        if len(L['text'].split()) > max_num_words or not1or5:\n",
    "            continue\n",
    "        if lowstar:\n",
    "            lowstar_counter += 1\n",
    "            L['lowstar'] = 1\n",
    "        else:\n",
    "            L['lowstar'] = 0\n",
    "        less_fields = {key: L[key] for key in review_fields_wanted }\n",
    "        rev.loc[counter] = less_fields\n",
    "        counter += 1\n",
    "        if counter == review_limit or lowstar_counter == lowstar_review_limit:\n",
    "            break\n",
    "\n",
    "            \n",
    "rev = rev.rename(columns = {'text':'_text','lowstar':'_lowstar'})\n",
    "\n",
    "\n",
    "rev = rev.groupby('_lowstar').apply(lambda x: x.sample(sample_per_cat)).reset_index(drop=True)\n",
    "rev['TARGETS'] = rev['_lowstar']\n",
    "print(f'Number of low star reviews:{rev._lowstar[rev._lowstar == 1].count()}')\n",
    "print(rev._text[rev._lowstar == 1].sample(5))\n",
    "print(f'Number of 5 star reviews:{rev._lowstar[rev._lowstar == 0].count()}')\n",
    "print(rev._text[rev._lowstar == 0].sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option to save the parsed reviews as a csv\n",
    "rev.to_csv('/home/pd/datasets/yelp_reviews/yelp_review_2048.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataset\n",
    "class DFToTokenized(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len):\n",
    "        self.len = len(df)\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        review = ' '.join(self.data['_text'][index].split())\n",
    "        inp = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        tokens = inp['input_ids']\n",
    "        mask = inp['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.TARGETS[index], dtype=torch.uint8)\n",
    "        } \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training params and dataloader\n",
    "- *MAX_LEN*: only allow reviews with token length less than this to be used. The\n",
    "token length is dictated by tokenization using the distilBERT vocabulary. Padding \n",
    "will then be performed to *max_len*.\n",
    "- *AUTO_SCALE_GRAD*: if true, this will automatically scale the precision of floats\n",
    "involved in gradient calculation to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init train/test params\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 1 #set to 1 for printing of individual wrong predictions\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "AUTO_SCALE_GRAD = False\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "train_frac = 0.8\n",
    "train_dataset=rev.sample(frac=train_frac,random_state=200)\n",
    "test_dataset=rev.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "training_set = DFToTokenized(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = DFToTokenized(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "single_params = {'batch_size': 1,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "single_loader = DataLoader(training_set,**single_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "distilBERT encoder's CLS token's hidden state is fed forward to eventually \n",
    "classify into positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBertMultiCat(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DBertMultiCat, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#INIT model, loss, optimizer\n",
    "model = DBertMultiCat()\n",
    "for p in model.l1.parameters():\n",
    "    p.requires_grad = False\n",
    "model.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "params_with_grad = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(params =  params_with_grad, lr=LEARNING_RATE)\n",
    "if AUTO_SCALE_GRAD:\n",
    "    scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop def\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    #for _,data in tqdm(enumerate(training_loader, 0),total=len(training_loader),\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.uint8)\n",
    "\n",
    "        if AUTO_SCALE_GRAD:\n",
    "            with torch.cpu.amp.autocast():\n",
    "                outputs = model(ids, mask)\n",
    "                loss = loss_function(outputs, targets)\n",
    "        else:\n",
    "            outputs = model(ids, mask)\n",
    "            loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if(AUTO_SCALE_GRAD):\n",
    "            scaler.scale(loss).backward()\n",
    "            # # When using GPU\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pd/NNBasics/venv_NNBasics/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.674589216709137\n",
      "Training Accuracy per 5000 steps: 59.375\n",
      "The Total Accuracy for Epoch 0: 49.93894993894994\n",
      "Training Loss Epoch: 0.696929796383931\n",
      "Training Accuracy Epoch: 49.93894993894994\n",
      "Training Loss per 5000 steps: 0.6952955722808838\n",
      "Training Accuracy per 5000 steps: 53.125\n",
      "The Total Accuracy for Epoch 1: 51.343101343101345\n",
      "Training Loss Epoch: 0.6932435700526605\n",
      "Training Accuracy Epoch: 51.343101343101345\n"
     ]
    }
   ],
   "source": [
    "#Run a few epochs for debugging before GPU train\n",
    "for epoch in range(2):\n",
    "    train(epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation loop definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation def\n",
    "def DBDetokenize(a):\n",
    "    a_orig = [tokenizer.decode(x) for x in a['ids'].squeeze().tolist() if x != 0]\n",
    "    a_orig = ([x.replace(' ' , '') for x in a_orig])\n",
    "    return \" \".join(a_orig)\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    tr_loss = 0 #added\n",
    "    nb_tr_steps = 0 #added\n",
    "    nb_tr_examples = 0 #added\n",
    "    max_wrong_outputs = 10\n",
    "    wrong_outputs = 0\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask)#.squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            #print individual wrong responses to file\n",
    "            if VALID_BATCH_SIZE == 1 and wrong_outputs < max_wrong_outputs: \n",
    "                wrong_outputs += 1\n",
    "                path = '/home/pd/summaries/yelp_summary_13Mar23.txt'\n",
    "                with open(path,'a') as f:\n",
    "                    f.write(DBDetokenize(data))\n",
    "                    f.write(f'Should be: {1 if targets.item() else 5}')\n",
    "                    f.write('\\n')\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 0.6816884279251099\n",
      "Validation Accuracy per 100 steps: 100.0\n",
      "Validation Loss Epoch: 0.6915209213408028\n",
      "Validation Accuracy Epoch: 50.48780487804878\n",
      "Accuracy on test data = 50.49%\n"
     ]
    }
   ],
   "source": [
    "#validation run\n",
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/pd/models/yelp_sentiment_vocab.bin',)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model_file = '/home/pd/models/yelp_sentiment.bin'\n",
    "output_vocab_file = '/home/pd/models/yelp_sentiment_vocab.bin'\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNBasics_venv2",
   "language": "python",
   "name": "nnbasics_venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54933d5d0454cb54ea8cf4e7b3c099269c704adbee2a5b35f5a5ea4d0f5219ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
