{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Transfer Learning for Sentiment Analysis\n",
    "**Objective:** Provide a framework to perform transfer learning using the pre-trained distilBERT model, \n",
    "allowing options for fine-tuning the distilBERT model or simply use its outputs as features. In this example, \n",
    "we use a dataset of Yelp reviews and build a sentiment classifier to identify whether a \n",
    "review is 1 or 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "device = 'cpu'\n",
    "device = 'gpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "**Summary:** parse Yelp reviews for the review text and number of stars associated with that review.\n",
    "Only parse reviews with low or high stars, and ensure that we have an equal number of low and high star reviews. Low star reviews have 1 or 2 stars, and high star reviews have 5 stars.<br /> \n",
    "- *lowstar_review_limit*: once we parse this number of low star reviews have been parsed. Break from processing. Typically there are more high star than low star reviews so the total number of reviews read in will be twice this number. <br /> \n",
    "- *review_limit*: once a total of this number of reviews have been parsed, stop reading in more.<br /> \n",
    "- *sample_per_cat*: sample this many low star and high star reviews respectively from what is parsed.<br /> \n",
    "- *max_num_words*: only parse reviews with number of words less than this length <br /> \n",
    "[Download the data](https://www.yelp.com/dataset/documentation/main) <br /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1 star reviews:1024\n",
      "Number of 5 star reviews:1024\n",
      "1142    Def,, not a place I\\nGot\\nFor\\nItalian\\nFood e...\n",
      "1715    Bad service, always out of things and rude whe...\n",
      "1070    I order the bone in ribeye fit market value. T...\n",
      "1284    I wish this place was more awesome. I love han...\n",
      "1397    I came here the other day and the person that ...\n",
      "Name: _text, dtype: object\n",
      "774    Best place late night.. authentic simple food....\n",
      "595    Finding out that your home will need new drain...\n",
      "915    Great food. Great atmosphere. The decor is rea...\n",
      "958    I visited the new Paoli location. BIG selectio...\n",
      "188    This place is fantastic! The ambiance is like ...\n",
      "Name: _text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#read-in data\n",
    "lowstar_review_limit = 1024\n",
    "review_limit = np.inf\n",
    "sample_per_cat = 1024\n",
    "max_num_words = 50\n",
    "\n",
    "\n",
    "path = \"/home/pd/datasets/yelp_reviews/yelp_reviews.json\"\n",
    "review_fields_wanted = ['text','lowstar']\n",
    "rev = pd.DataFrame(columns=review_fields_wanted)\n",
    "with open(path,encoding='utf-8') as d:\n",
    "    counter = 0\n",
    "    lowstar_counter = 0\n",
    "    for line in d:\n",
    "        L = json.loads(line)\n",
    "        lowstar = L['stars'] == 1 or (L['stars'] == 2)\n",
    "        fivestar = L['stars'] == 5\n",
    "        not1or5 = not(lowstar or fivestar)\n",
    "        if len(L['text'].split()) > max_num_words or not1or5:\n",
    "            continue\n",
    "        if lowstar:\n",
    "            lowstar_counter += 1\n",
    "            L['lowstar'] = 1\n",
    "        else:\n",
    "            L['lowstar'] = 0\n",
    "        less_fields = {key: L[key] for key in review_fields_wanted }\n",
    "        rev.loc[counter] = less_fields\n",
    "        counter += 1\n",
    "        if counter == review_limit or lowstar_counter == lowstar_review_limit:\n",
    "            break\n",
    "\n",
    "            \n",
    "rev = rev.rename(columns = {'text':'_text','lowstar':'_lowstar'})\n",
    "\n",
    "\n",
    "rev = rev.groupby('_lowstar').apply(lambda x: x.sample(sample_per_cat)).reset_index(drop=True)\n",
    "rev['TARGETS'] = rev['_lowstar']\n",
    "print(f'Number of 1 star reviews:{rev._lowstar[rev._lowstar == 1].count()}')\n",
    "print(f'Number of 5 star reviews:{rev._lowstar[rev._lowstar == 0].count()}')\n",
    "print(rev._text[rev._lowstar == 1].sample(5))\n",
    "print(rev._text[rev._lowstar == 0].sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option to save the parsed reviews as a csv\n",
    "rev.to_csv('/home/pd/datasets/yelp_reviews/yelp_review_2048.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataset\n",
    "class DFToTokenized(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len):\n",
    "        self.len = len(df)\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        review = ' '.join(self.data['_text'][index].split())\n",
    "        inp = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        tokens = inp['input_ids']\n",
    "        mask = inp['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.TARGETS[index], dtype=torch.uint8)\n",
    "        } \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training params and dataloader\n",
    "- *MAX_LEN*: only allow reviews with token length less than this to be used. The\n",
    "token length is dictated by tokenization using the distilBERT vocabulary. Padding \n",
    "will then be performed to *max_len*.\n",
    "- *AUTO_SCALE_GRAD*: if true, this will automatically scale the precision of floats\n",
    "involved in gradient calculation to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init train/test params\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "AUTO_SCALE_GRAD = False\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "train_frac = 0.8\n",
    "train_dataset=rev.sample(frac=train_frac,random_state=200)\n",
    "test_dataset=rev.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "training_set = DFToTokenized(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = DFToTokenized(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "single_params = {'batch_size': 1,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "single_loader = DataLoader(training_set,**single_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "distilBERT encoder's CLS token's hidden state is fed forward to eventually \n",
    "classify into positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBertMultiCat(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DBertMultiCat, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#INIT model, loss, optimizer\n",
    "model = DBertMultiCat()\n",
    "for p in model.l1.parameters():\n",
    "    p.requires_grad = False\n",
    "model.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "params_with_grad = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(params =  params_with_grad, lr=LEARNING_RATE)\n",
    "if AUTO_SCALE_GRAD:\n",
    "    scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop def\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0),total=len(training_loader),\n",
    "        position=0, leave=True):\n",
    "\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.uint8)\n",
    "\n",
    "        if AUTO_SCALE_GRAD:\n",
    "            with torch.cpu.amp.autocast():\n",
    "                outputs = model(ids, mask)\n",
    "                loss = loss_function(outputs, targets)\n",
    "        else:\n",
    "            outputs = model(ids, mask)\n",
    "            loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if(AUTO_SCALE_GRAD):\n",
    "            scaler.scale(loss).backward()\n",
    "            # # When using GPU\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<00:55,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.687899649143219\n",
      "Training Accuracy per 5000 steps: 62.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:02<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 0: 52.747252747252745\n",
      "Training Loss Epoch: 0.6927502017754775\n",
      "Training Accuracy Epoch: 52.747252747252745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:06,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6897991299629211\n",
      "Training Accuracy per 5000 steps: 51.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:10<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 1: 53.96825396825397\n",
      "Training Loss Epoch: 0.6895589438768533\n",
      "Training Accuracy Epoch: 53.96825396825397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.694517195224762\n",
      "Training Accuracy per 5000 steps: 54.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:19<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 2: 56.59340659340659\n",
      "Training Loss Epoch: 0.6890195699838492\n",
      "Training Accuracy Epoch: 56.59340659340659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:13,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.7080498337745667\n",
      "Training Accuracy per 5000 steps: 40.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:16<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 3: 55.98290598290598\n",
      "Training Loss Epoch: 0.6872568657765021\n",
      "Training Accuracy Epoch: 55.98290598290598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:12,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6820058822631836\n",
      "Training Accuracy per 5000 steps: 57.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 4: 53.84615384615385\n",
      "Training Loss Epoch: 0.6881257914579831\n",
      "Training Accuracy Epoch: 53.84615384615385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6906676292419434\n",
      "Training Accuracy per 5000 steps: 54.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 5: 56.28815628815629\n",
      "Training Loss Epoch: 0.6852830465023334\n",
      "Training Accuracy Epoch: 56.28815628815629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6812927722930908\n",
      "Training Accuracy per 5000 steps: 59.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:30<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 6: 55.61660561660562\n",
      "Training Loss Epoch: 0.6840876249166635\n",
      "Training Accuracy Epoch: 55.61660561660562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:10,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6842745542526245\n",
      "Training Accuracy per 5000 steps: 54.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 7: 57.32600732600733\n",
      "Training Loss Epoch: 0.6829985403097593\n",
      "Training Accuracy Epoch: 57.32600732600733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:08,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6744179725646973\n",
      "Training Accuracy per 5000 steps: 62.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 8: 56.65445665445665\n",
      "Training Loss Epoch: 0.6826992126611563\n",
      "Training Accuracy Epoch: 56.65445665445665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:10,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6916084289550781\n",
      "Training Accuracy per 5000 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 9: 58.42490842490842\n",
      "Training Loss Epoch: 0.6816483438014984\n",
      "Training Accuracy Epoch: 58.42490842490842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:12,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6947983503341675\n",
      "Training Accuracy per 5000 steps: 45.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 10: 57.753357753357754\n",
      "Training Loss Epoch: 0.6786751540807577\n",
      "Training Accuracy Epoch: 57.753357753357754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6780788898468018\n",
      "Training Accuracy per 5000 steps: 57.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 11: 58.66910866910867\n",
      "Training Loss Epoch: 0.6798896376903241\n",
      "Training Accuracy Epoch: 58.66910866910867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6760585308074951\n",
      "Training Accuracy per 5000 steps: 62.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:19<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 12: 58.73015873015873\n",
      "Training Loss Epoch: 0.6783169141182532\n",
      "Training Accuracy Epoch: 58.73015873015873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:12,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6931607723236084\n",
      "Training Accuracy per 5000 steps: 45.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:14<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 13: 58.05860805860806\n",
      "Training Loss Epoch: 0.6764536568751702\n",
      "Training Accuracy Epoch: 58.05860805860806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6862077713012695\n",
      "Training Accuracy per 5000 steps: 60.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:14<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 14: 59.95115995115995\n",
      "Training Loss Epoch: 0.6758655126278217\n",
      "Training Accuracy Epoch: 59.95115995115995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:12,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6996618509292603\n",
      "Training Accuracy per 5000 steps: 46.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 15: 59.584859584859586\n",
      "Training Loss Epoch: 0.6741755237946143\n",
      "Training Accuracy Epoch: 59.584859584859586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6771283149719238\n",
      "Training Accuracy per 5000 steps: 57.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 16: 60.195360195360195\n",
      "Training Loss Epoch: 0.6739947497844696\n",
      "Training Accuracy Epoch: 60.195360195360195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:10,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6989840269088745\n",
      "Training Accuracy per 5000 steps: 53.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:12<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 17: 60.317460317460316\n",
      "Training Loss Epoch: 0.6729006423399999\n",
      "Training Accuracy Epoch: 60.317460317460316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:14,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6668469905853271\n",
      "Training Accuracy per 5000 steps: 59.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:12<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 18: 60.62271062271062\n",
      "Training Loss Epoch: 0.6734807514227353\n",
      "Training Accuracy Epoch: 60.62271062271062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:09,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.651386022567749\n",
      "Training Accuracy per 5000 steps: 65.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:11<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 19: 58.97435897435897\n",
      "Training Loss Epoch: 0.6731648949476389\n",
      "Training Accuracy Epoch: 58.97435897435897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:08,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.7033386826515198\n",
      "Training Accuracy per 5000 steps: 46.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:12<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 20: 60.866910866910864\n",
      "Training Loss Epoch: 0.6696507357634031\n",
      "Training Accuracy Epoch: 60.866910866910864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:08,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6836097240447998\n",
      "Training Accuracy per 5000 steps: 54.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:12<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 21: 60.866910866910864\n",
      "Training Loss Epoch: 0.6700746646294227\n",
      "Training Accuracy Epoch: 60.866910866910864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:10,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6750324964523315\n",
      "Training Accuracy per 5000 steps: 60.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 22: 59.15750915750916\n",
      "Training Loss Epoch: 0.6726355460973886\n",
      "Training Accuracy Epoch: 59.15750915750916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:12,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6852738261222839\n",
      "Training Accuracy per 5000 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 23: 59.34065934065934\n",
      "Training Loss Epoch: 0.6697726226769961\n",
      "Training Accuracy Epoch: 59.34065934065934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:12,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6852605938911438\n",
      "Training Accuracy per 5000 steps: 54.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:14<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 24: 61.35531135531136\n",
      "Training Loss Epoch: 0.6651676939083979\n",
      "Training Accuracy Epoch: 61.35531135531136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.66957026720047\n",
      "Training Accuracy per 5000 steps: 67.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:13<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 25: 59.21855921855922\n",
      "Training Loss Epoch: 0.6684628656277289\n",
      "Training Accuracy Epoch: 59.21855921855922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:13,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6743199825286865\n",
      "Training Accuracy per 5000 steps: 62.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:16<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 26: 61.172161172161175\n",
      "Training Loss Epoch: 0.6654344430336585\n",
      "Training Accuracy Epoch: 61.172161172161175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:02<01:11,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6587346792221069\n",
      "Training Accuracy per 5000 steps: 70.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:18<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 27: 61.233211233211236\n",
      "Training Loss Epoch: 0.6644795330671164\n",
      "Training Accuracy Epoch: 61.233211233211236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:05<02:11,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6700491905212402\n",
      "Training Accuracy per 5000 steps: 59.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [03:18<00:00,  7.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 28: 62.27106227106227\n",
      "Training Loss Epoch: 0.6637566593977121\n",
      "Training Accuracy Epoch: 62.27106227106227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:23<09:54, 23.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.7042104601860046\n",
      "Training Accuracy per 5000 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [07:04<00:00, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 29: 59.70695970695971\n",
      "Training Loss Epoch: 0.666911624945127\n",
      "Training Accuracy Epoch: 59.70695970695971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Run the training\n",
    "for epoch in range(30):\n",
    "    train(epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation loop definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation def\n",
    "VALID_BATCH_SIZE = 1 #set to 1 for printing of individual wrong predictions\n",
    "\n",
    "#detokenize def\n",
    "def DBDetokenize(a):\n",
    "    a_orig = [tokenizer.decode(x) for x in a['ids'].squeeze().tolist() if x != 0]\n",
    "    a_orig = ([x.replace(' ' , '') for x in a_orig])\n",
    "    return \" \".join(a_orig)\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    tr_loss = 0 #added\n",
    "    nb_tr_steps = 0 #added\n",
    "    nb_tr_examples = 0 #added\n",
    "    max_wrong_outputs = 10\n",
    "    wrong_outputs = 0\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask)#.squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            #print individual wrong responses to file\n",
    "            if VALID_BATCH_SIZE == 1 and wrong_outputs < max_wrong_outputs: \n",
    "                wrong_outputs += 1\n",
    "                path = '/home/pd/summaries/yelp_summary_13Mar23.txt'\n",
    "                with open(path,'a') as f:\n",
    "                    f.write(DBDetokenize(data))\n",
    "                    f.write(f'Should be: {1 if targets.item() else 5}')\n",
    "                    f.write('\\n')\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pd/NNBasics/venv_NNBasics/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 0.4546707570552826\n",
      "Validation Accuracy per 100 steps: 100.0\n",
      "Validation Loss Epoch: 0.6451624179395233\n",
      "Validation Accuracy Epoch: 64.8780487804878\n",
      "Accuracy on test data = 64.88%\n"
     ]
    }
   ],
   "source": [
    "#validation run\n",
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/pd/models/yelp_sentiment_vocab.bin',)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model_file = '/home/pd/models/yelp_sentiment.bin'\n",
    "output_vocab_file = '/home/pd/models/yelp_sentiment_vocab.bin'\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNBasics_venv2",
   "language": "python",
   "name": "nnbasics_venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54933d5d0454cb54ea8cf4e7b3c099269c704adbee2a5b35f5a5ea4d0f5219ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
