{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Transfer Learning for Sentiment Analysis\n",
    "**Objective:** Provide a framework to perform transfer learning using the pre-trained distilBERT model, \n",
    "allowing options for fine-tuning the distilBERT model or simply use its outputs as features. In this example, \n",
    "we use a dataset of Yelp reviews and build a sentiment classifier to identify whether a \n",
    "review is 1 or 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "**Summary:** parse Yelp reviews for the review text and number of stars associated with that review.\n",
    "Only parse reviews with low or high stars, and ensure that we have an equal number of low and high star reviews. Low star reviews have 1 or 2 stars, and high star reviews have 5 stars.<br /> \n",
    "- *lowstar_review_limit*: once we parse this number of low star reviews have been parsed. Break from processing. Typically there are more high star than low star reviews so the total number of reviews read in will be twice this number. <br /> \n",
    "- *review_limit*: once a total of this number of reviews have been parsed, stop reading in more.<br /> \n",
    "- *sample_per_cat*: sample this many low star and high star reviews respectively from what is parsed.<br /> \n",
    "- *max_num_words*: only parse reviews with number of words less than this length <br /> \n",
    "- Note that here we provide training results over a small dataset using the CPU so that quick testing and tuning can occur before scaling to larger compute resources on a GPU.\n",
    "[Download the data](https://www.yelp.com/dataset/documentation/main) <br /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of low star reviews:1024\n",
      "1457    Very disappointed. Store hours online need to ...\n",
      "1085    The staff was very friendly and the food was q...\n",
      "1699    Nice outdoor atmosphere, big problem with no w...\n",
      "1195    Everything is great except the A/C kicks at 10...\n",
      "1337    Decent place to get a drink and usual diner/ba...\n",
      "Name: _text, dtype: object\n",
      "Number of 5 star reviews:1024\n",
      "793    Been a long time since I've had a local Cuban ...\n",
      "913    Been here twice. Really pleased both times. Ch...\n",
      "937    Best pizza I have ever had! Delivery is always...\n",
      "803    Cozy place on charter street.\\nMy burger was p...\n",
      "395    Very friendly, professional, and helpful peopl...\n",
      "Name: _text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#read-in data\n",
    "lowstar_review_limit = 1024\n",
    "review_limit = np.inf\n",
    "sample_per_cat = 1024\n",
    "max_num_words = 50\n",
    "\n",
    "\n",
    "path = \"/home/pd/datasets/yelp_reviews/yelp_reviews.json\"\n",
    "review_fields_wanted = ['text','lowstar']\n",
    "rev = pd.DataFrame(columns=review_fields_wanted)\n",
    "with open(path,encoding='utf-8') as d:\n",
    "    counter = 0\n",
    "    lowstar_counter = 0\n",
    "    for line in d:\n",
    "        L = json.loads(line)\n",
    "        lowstar = L['stars'] == 1 or (L['stars'] == 2)\n",
    "        fivestar = L['stars'] == 5\n",
    "        not1or5 = not(lowstar or fivestar)\n",
    "        if len(L['text'].split()) > max_num_words or not1or5:\n",
    "            continue\n",
    "        if lowstar:\n",
    "            lowstar_counter += 1\n",
    "            L['lowstar'] = 1\n",
    "        else:\n",
    "            L['lowstar'] = 0\n",
    "        less_fields = {key: L[key] for key in review_fields_wanted }\n",
    "        rev.loc[counter] = less_fields\n",
    "        counter += 1\n",
    "        if counter == review_limit or lowstar_counter == lowstar_review_limit:\n",
    "            break\n",
    "\n",
    "            \n",
    "rev = rev.rename(columns = {'text':'_text','lowstar':'_lowstar'})\n",
    "\n",
    "\n",
    "rev = rev.groupby('_lowstar').apply(lambda x: x.sample(sample_per_cat)).reset_index(drop=True)\n",
    "rev['TARGETS'] = rev['_lowstar']\n",
    "print(f'Number of low star reviews:{rev._lowstar[rev._lowstar == 1].count()}')\n",
    "print(rev._text[rev._lowstar == 1].sample(5))\n",
    "print(f'Number of 5 star reviews:{rev._lowstar[rev._lowstar == 0].count()}')\n",
    "print(rev._text[rev._lowstar == 0].sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option to save the parsed reviews as a csv\n",
    "rev.to_csv('/home/pd/datasets/yelp_reviews/yelp_review_2048.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataset\n",
    "class DFToTokenized(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len):\n",
    "        self.len = len(df)\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        review = ' '.join(self.data['_text'][index].split())\n",
    "        inp = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        tokens = inp['input_ids']\n",
    "        mask = inp['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.TARGETS[index], dtype=torch.uint8)\n",
    "        } \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training params and dataloader\n",
    "- *MAX_LEN*: only allow reviews with token length less than this to be used. The\n",
    "token length is dictated by tokenization using the distilBERT vocabulary. Padding \n",
    "will then be performed to *max_len*.\n",
    "- *AUTO_SCALE_GRAD*: if true, this will automatically scale the precision of floats\n",
    "involved in gradient calculation to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init train/test params\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 1 #set to 1 for printing of individual wrong predictions\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "AUTO_SCALE_GRAD = False\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "train_frac = 0.8\n",
    "train_dataset=rev.sample(frac=train_frac,random_state=200)\n",
    "test_dataset=rev.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "training_set = DFToTokenized(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = DFToTokenized(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "single_params = {'batch_size': 1,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "single_loader = DataLoader(training_set,**single_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "distilBERT encoder's CLS token's hidden state is fed forward to eventually \n",
    "classify into positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBertMultiCat(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DBertMultiCat, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INIT model, loss, optimizer\n",
    "model = DBertMultiCat()\n",
    "for p in model.l1.parameters():\n",
    "    p.requires_grad = False\n",
    "model.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "params_with_grad = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(params =  params_with_grad, lr=LEARNING_RATE)\n",
    "if AUTO_SCALE_GRAD:\n",
    "    scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop def\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    #for _,data in tqdm(enumerate(training_loader, 0),total=len(training_loader),\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.uint8)\n",
    "\n",
    "        if AUTO_SCALE_GRAD:\n",
    "            with torch.cpu.amp.autocast():\n",
    "                outputs = model(ids, mask)\n",
    "                loss = loss_function(outputs, targets)\n",
    "        else:\n",
    "            outputs = model(ids, mask)\n",
    "            loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if(AUTO_SCALE_GRAD):\n",
    "            scaler.scale(loss).backward()\n",
    "            # # When using GPU\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pd/NNBasics/venv_NNBasics/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6978678703308105\n",
      "Training Accuracy per 5000 steps: 56.25\n",
      "The Total Accuracy for Epoch 0: 50.366300366300365\n",
      "Training Loss Epoch: 0.6941294211607713\n",
      "Training Accuracy Epoch: 50.366300366300365\n",
      "Training Loss per 5000 steps: 0.6917526721954346\n",
      "Training Accuracy per 5000 steps: 50.0\n"
     ]
    }
   ],
   "source": [
    "#Run the training\n",
    "for epoch in range(30):\n",
    "    train(epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation loop definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation def\n",
    "def DBDetokenize(a):\n",
    "    a_orig = [tokenizer.decode(x) for x in a['ids'].squeeze().tolist() if x != 0]\n",
    "    a_orig = ([x.replace(' ' , '') for x in a_orig])\n",
    "    return \" \".join(a_orig)\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    tr_loss = 0 #added\n",
    "    nb_tr_steps = 0 #added\n",
    "    nb_tr_examples = 0 #added\n",
    "    max_wrong_outputs = 10\n",
    "    wrong_outputs = 0\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask)#.squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            #print individual wrong responses to file\n",
    "            if VALID_BATCH_SIZE == 1 and wrong_outputs < max_wrong_outputs: \n",
    "                wrong_outputs += 1\n",
    "                path = '/home/pd/summaries/yelp_summary_13Mar23.txt'\n",
    "                with open(path,'a') as f:\n",
    "                    f.write(DBDetokenize(data))\n",
    "                    f.write(f'Should be: {1 if targets.item() else 5}')\n",
    "                    f.write('\\n')\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb Cell 18\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#validation run\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m acc \u001b[39m=\u001b[39m valid(model, testing_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy on test data = \u001b[39m\u001b[39m%0.2f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m acc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#validation run\n",
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb Cell 19\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m output_model_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/pd/models/yelp_sentiment.bin\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m output_vocab_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/pd/models/yelp_sentiment_vocab.bin\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model_to_save \u001b[39m=\u001b[39m model\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model_to_save, output_model_file)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/pd/NNBasics/NNBasics/notebooks/Transformer_Sentiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer\u001b[39m.\u001b[39msave_vocabulary(output_vocab_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "output_model_file = '/home/pd/models/yelp_sentiment.bin'\n",
    "output_vocab_file = '/home/pd/models/yelp_sentiment_vocab.bin'\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNBasics_venv2",
   "language": "python",
   "name": "nnbasics_venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54933d5d0454cb54ea8cf4e7b3c099269c704adbee2a5b35f5a5ea4d0f5219ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
